12/29/2020 09:20:12 - INFO - transformers.training_args -   PyTorch: setting up devices
12/29/2020 09:20:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
12/29/2020 09:20:12 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-hotflip', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=True, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=35.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec29_09-20-11_gpower_a', logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)
12/29/2020 09:20:12 - INFO - transformers.configuration_utils -   loading configuration file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/config.json
12/29/2020 09:20:12 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-bio",
    "2": "I-bio",
    "3": "-bio"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "-bio": 3,
    "B-bio": 1,
    "I-bio": 2,
    "O": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

12/29/2020 09:20:12 - INFO - transformers.configuration_utils -   loading configuration file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/config.json
12/29/2020 09:20:12 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B",
    "2": "I",
    "3": "X"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B": 1,
    "I": 2,
    "O": 0,
    "X": 3
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   Model name '/hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot' is a path, a model identifier, or url to a directory containing tokenizer files.
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   Didn't find file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/added_tokens.json. We won't load it.
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   Didn't find file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/tokenizer.json. We won't load it.
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   loading file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/vocab.txt
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   loading file None
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   loading file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/special_tokens_map.json
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   loading file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/tokenizer_config.json
12/29/2020 09:20:12 - INFO - transformers.tokenization_utils_base -   loading file None
12/29/2020 09:20:12 - INFO - transformers.modeling_utils -   loading weights file /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot/pytorch_model.bin
12/29/2020 09:20:14 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing BertForTokenClassification.

12/29/2020 09:20:14 - INFO - transformers.modeling_utils -   All the weights of BertForTokenClassification were initialized from the model checkpoint at /hdd1/minbyul/output/ner_output//biobert/NCBI-disease-softmax-35-removedot.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForTokenClassification for predictions without further training.
12/29/2020 09:20:14 - INFO - filelock -   Lock 140311828308304 acquired on /home/minbyul/github/biobert-pytorch/datasets//NER/NCBI-disease/cached_devel_BertTokenizer_128.lock
12/29/2020 09:20:14 - INFO - utils.utils_ner -   Loading features from cached file /home/minbyul/github/biobert-pytorch/datasets//NER/NCBI-disease/cached_devel_BertTokenizer_128
12/29/2020 09:20:14 - INFO - filelock -   Lock 140311828308304 released on /home/minbyul/github/biobert-pytorch/datasets//NER/NCBI-disease/cached_devel_BertTokenizer_128.lock
12/29/2020 09:20:14 - INFO - filelock -   Lock 140311828301520 acquired on /home/minbyul/github/biobert-pytorch/datasets//NER/NCBI-disease/cached_test_BertTokenizer_128.lock
12/29/2020 09:20:14 - INFO - utils.utils_ner -   Loading features from cached file /home/minbyul/github/biobert-pytorch/datasets//NER/NCBI-disease/cached_test_BertTokenizer_128
12/29/2020 09:20:14 - INFO - filelock -   Lock 140311828301520 released on /home/minbyul/github/biobert-pytorch/datasets//NER/NCBI-disease/cached_test_BertTokenizer_128.lock
12/29/2020 09:20:16 - INFO - __main__ -   *** Adversarial attack on Evaluation Set***
  0%|          | 0/923 [00:00<?, ?it/s]> /home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py(103)_adversarial_tokens()
-> best_positions = torch.clamp(best_positions, 0, max_length-1)
(Pdb) tensor([[3]], device='cuda:0')
(Pdb) 1
(Pdb) > /home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py(106)_adversarial_tokens()
-> pdb.set_trace()
(Pdb) tensor([[3]], device='cuda:0')
(Pdb) 17
(Pdb) > /home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py(110)_adversarial_tokens()
-> src = best_at_each_step.gather(dim=1, index=best_positions.clone())
(Pdb) tensor([[  101, 26660, 11356,  1475,  1110,  3318,  1174,  1105, 10877,  4625,
          1104,   170,   176, 23851,  1179,   119,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]],
       device='cuda:0')
(Pdb) *** NameError: name 'index' is not defined
(Pdb) > /home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py(114)_adversarial_tokens()
-> pdb.set_trace()
(Pdb) tensor([[3]], device='cuda:0')
(Pdb) tensor([[1475]], device='cuda:0')
(Pdb) > /home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py(116)_adversarial_tokens()
(Pdb) tensor([[  101, 26660, 11356,  1475,  1110,  3318,  1174,  1105, 10877,  4625,
          1104,   170,   176, 23851,  1179,   119,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]],
       device='cuda:0')
(Pdb) tensor([[  101, 26660, 11356,  1475,  1110,  3318,  1174,  1105, 10877,  4625,
          1104,   170,   176, 23851,  1179,   119,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]],
       device='cuda:0')
(Pdb) tensor([[2.9757e-04, 1.8507e-04, 1.7615e-04, 1.3097e+00, 2.6075e-04, 1.1642e+00,
         4.6732e-04, 7.2760e-02, 6.0644e-04, 4.3162e-04, 9.4587e-01, 2.4715e-04,
         7.2760e-02, 7.1805e-04, 2.0412e-04, 1.9858e-04, 2.2105e-05, 2.2339e-06,
         1.9789e-07, 2.2158e-07, 1.7769e-05, 5.9117e-02, 1.2353e-04, 5.6843e-04,
         1.2103e-06, 4.2633e-04, 3.1974e-04, 7.1054e-05, 4.5475e-02, 2.6971e-04,
         4.3774e-05, 7.5031e-08, 1.7715e-07, 1.6688e-06, 8.7884e-05, 1.4299e-06,
         3.5871e-07, 2.8404e-05, 3.4106e-02, 3.4106e-03, 3.3206e-07, 5.4550e-06,
         1.8248e-05, 7.8885e-07, 1.7242e-07, 2.5424e-06, 6.9987e-08, 1.2637e-06,
         2.9749e-05, 8.8818e-06, 6.3254e-09, 2.6149e-09, 7.1054e-05, 5.6843e-04,
         3.5527e-04, 1.0298e-07, 4.4144e-07, 1.1683e-06, 3.5828e-06, 1.0893e-06,
         9.4147e-04, 8.8818e-05, 2.3781e-06, 5.6843e-04, 7.2760e-02, 1.0330e-04,
         3.5527e-05, 3.1306e-08, 7.1054e-05, 5.6843e-04, 4.6739e-05, 6.8212e-03,
         2.8422e-04, 8.9380e-07, 7.2617e-07, 1.0658e-04, 2.1272e-07, 5.6830e-07,
         7.5370e-07, 5.6843e-04, 1.4211e-04, 9.1599e-07, 2.2204e-05, 4.6016e-07,
         2.4869e-04, 3.6124e-07, 1.9482e-07, 1.6577e-08, 5.3291e-05, 1.3642e-02,
         2.8422e-04, 1.4211e-04, 2.2164e-04, 3.1974e-04, 3.3723e-07, 1.5698e-05,
         6.9344e-08, 1.4211e-04, 1.4089e-06, 3.1376e-08, 1.5333e-07, 1.2517e-05,
         2.7288e-09, 3.1360e-07, 5.6390e-09, 1.1436e-08, 4.5475e-03, 7.7915e-05,
         1.7724e-04, 2.9270e-08, 1.8263e-05, 1.1642e+00, 3.3511e-06, 4.3293e-07,
         8.5265e-04, 5.0510e-07, 2.9495e-06, 1.0435e-07, 3.9424e-06, 2.2737e-03,
         2.0008e-06, 2.1316e-04, 1.9239e-06, 2.2861e-04, 8.8818e-06, 6.2172e-05,
         3.3697e-09, 4.3948e-06]], device='cuda:0')
(Pdb) tensor([[  101, 26660, 11356,  1475,  1110,  3318,  1174,  1105, 10877,  4625,
          1104,   170,   176, 23851,  1179,   119,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]],
       device='cuda:0')
(Pdb) tensor([[  101, 26660, 11356,  1475,  1110,  3318,  1174,  1105, 10877,  4625,
          1104,   170,   176, 23851,  1179,   119,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]],
       device='cuda:0')
(Pdb) tensor([[1475]], device='cuda:0')
(Pdb) (Pdb) tensor([[3]], device='cuda:0')
(Pdb) tensor([[3]], device='cuda:0')
(Pdb) *** SyntaxError: unexpected EOF while parsing
(Pdb) tensor([[3]], device='cuda:0')
(Pdb)   0%|          | 0/923 [1:15:42<?, ?it/s]
Traceback (most recent call last):
  File "run_ner.py", line 281, in <module>
    main()
  File "run_ner.py", line 256, in main
    instances = adversarial.adversarial_attack(eval_dataset)
  File "/home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py", line 26, in adversarial_attack
    self._adversarial_tokens(batch)
  File "/home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py", line 116, in _adversarial_tokens
  File "/home/minbyul/github/biobert-pytorch/adversarial-attack/hotflip.py", line 116, in _adversarial_tokens
  File "/home/minbyul/anaconda3/envs/biobert-pytorch/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/minbyul/anaconda3/envs/biobert-pytorch/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
